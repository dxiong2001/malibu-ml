{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "html_parser.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOyyqhBnL1ym+CtWD08RNSP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dxiong2001/malibu-ml/blob/main/html_parser.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUeKeA6ir3sk",
        "outputId": "0beebffb-b5d4-4c89-a7d8-d168e84551dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "#imports \n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import re\n",
        "import unicodedata\n",
        "import spacy\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_html(url):\n",
        "  page = requests.get(url)\n",
        "  return BeautifulSoup(page.content, 'html.parser')\n",
        "\n",
        "\n",
        "def getArticleTextSections(page_content):\n",
        "  body_text = page_content.find('div', class_='Article-bodyText')\n",
        "  body_text_p = body_text.find_all('p', class_='')\n",
        "\n",
        "  processed_text1 = [item.get_text() for item in body_text_p]\n",
        "  processed_text2=[]\n",
        "  for processed in processed_text1:\n",
        "    p2 = processed.replace('“', '\"').replace('”', '\"').replace(\"‘\", \"'\").replace(\"’\", \"'\").replace('…', '...').replace('–', '-')\n",
        "    processed_text2.append(unicodedata.normalize('NFKD', p2))\n",
        "  \n",
        "  return processed_text2\n",
        "  \n",
        "\n",
        "def getCombinedText(processed_text):\n",
        "  sentences = [sent_tokenize(sent) for sent in processed_text]\n",
        "\n",
        "  combined_sentences = []\n",
        "  for sent in sentences:\n",
        "    combined_sentences += sent\n",
        "\n",
        "  return combined_sentences\n",
        "\n",
        "\n",
        "def getQuotes(processed_text):\n",
        "  combined_sentences = getCombinedText(processed_text)\n",
        "\n",
        "  quotes_incomplete = [re.findall('\"([^\"]*)\"', paragraph) for paragraph in processed_text]\n",
        "  quotes_incomplete = [q for q in quotes_incomplete if q != []]\n",
        "  quotes_incomplete2 = [j for i in quotes_incomplete for j in i]\n",
        "\n",
        "  quotes_complete =[]\n",
        "\n",
        "  i=0\n",
        "  for comb in combined_sentences:\n",
        "    \n",
        "    if(quotes_incomplete2[i] in comb):\n",
        "      if(comb not in quotes_complete):\n",
        "        quotes_complete.append(comb)\n",
        "      i = i + 1\n",
        "      while i < len(quotes_incomplete2) and quotes_incomplete2[i] in comb:\n",
        "          i = i + 1\n",
        "      \n",
        "    \n",
        "    if(i==len(quotes_incomplete2)):\n",
        "        break\n",
        "\n",
        "  return quotes_complete\n",
        "\n",
        "def getArticleInfo(page_content):\n",
        "\n",
        "  #grab article information\n",
        "  header = page_content.find('div', class_='Article-header')\n",
        "  article_info = header.find('p', class_='Article-author').get_text()\n",
        "  title = header.find('h1', class_='u-entryTitle').get_text()\n",
        "  author = article_info.split(\"|\")[0].replace(\"\\n                    \",\"\").replace(\"\\n              \",\"\")[2:]\n",
        "  date = article_info.split(\"|\")[1].replace(\"\\n    \\n    Updated \",\"\").replace(\"\\n  \\n\",\"\")\n",
        "\n",
        "  return title, author, date\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def getNamedEntities(processed_text):\n",
        "  joined_sentences = ' '.join(getCombinedText(processed_text))\n",
        "  print(joined_sentences)\n",
        "  nlp_processed_text = nlp(joined_sentences)\n",
        "\n",
        "  named_entities=[]\n",
        "  for entities in nlp_processed_text.ents:\n",
        "    named_entities.append((entities.text, entities.label_))\n",
        "    print(entities.text, entities.label_)\n",
        "\n",
        "  people = []\n",
        "  for n in named_entities:\n",
        "    if(n[1]==\"PERSON\" and \" \" in n[0] and not any(i.isdigit() for i in n[0])):\n",
        "      people.append(n[0])\n",
        "\n",
        "  people_extended = []\n",
        "  for p in range(len(people)):\n",
        "    temp = []\n",
        "    temp.append(people[p])\n",
        "    temp = temp+people[p].split()\n",
        "    people_extended.append(temp)\n",
        "    \n",
        "  return people, people_extended"
      ],
      "metadata": {
        "id": "R9ViaoDPsRM6"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}